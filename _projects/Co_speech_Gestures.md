---
layout: page
title: Co-speech Gestures in Language Disorder
description: Aphasia, MCI, ASR
img: /assets/img/projects_preview/p2_img.jpg
importance: 1
related_publications: true
---

## Research Background
Multimodal language analysis helps to understand human communication by integrating information from multiple modalities.
However, previous studies simply concatenated features to identify language impairment in speech data, which lacks comprehension of the complex connections between modalities.

Individuals with language disorder often rely on non-verbal communication techniques, especially gestures, as an additional communication tool due to difficulties in word retrieval and language errors.
Therefore, the same word can be interpreted differently depending on the accompanying gestures for different language disorder symptoms. Hence, utilizing both speech (i.e., linguistic and acoustic) and gesture (i.e., visual) information is crucial in understanding language disorder's characteristics.

<img src="/assets/img/projects_preview/p1_socialmedia.png" width="80%" title="p2_ex"/>

## Research Goal
We aims to establish healthcare systems based on understanding language disorder's characteristics by utilizing both speech (i.e., linguistic and acoustic) and gesture (i.e., visual) information.


## Approach
- __Understanding Co-Speech Gestures for Aphasia Type Detection__ {% cite lee2023learning %}: 

    Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. We show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types.

- __Using Audiovisual Features for Depression Detection__ {% cite min2023detecting %}:

    We collected vlogs from YouTube and annotated them into depression and non-depression. Based on analysis of  the statistical differences between depression and non-depression vlogs, we build a depression detection model that learns both audio and visual features, achieving high accuracy.

- __Analysis for Multi-modality for MCI Detection__ _(~ing)_:

    I am actively involved in this project related to mild cognitive impairment detection with domain experts, such as pathologists and healthcare practitioners at the University of South Florida (USF).

    ---